"""
Create BioASQ factoid test-only dataset from local parquet files.

This script reads the train/test split parquet files generated by
process_bioasq_golden_answers.py and combines them into a single
test-only dataset (as used in the paper).

PREREQUISITES:
    1. Run process_bioasq_golden_answers.py first to generate the parquet files
    2. This will create files in output/ directory

This script:
1. Loads train and test parquet files from output/ directory
2. Combines all samples into a single test split
3. Saves as test-only dataset
4. Optionally uploads to HuggingFace if ENABLE_HF_UPLOAD=1

Usage:
    python create_bioasq_factoid.py --source_dataset bioasq_trainv0_n1609
"""

import os
import argparse
import pandas as pd
from datasets import Dataset, DatasetDict
from typing import List, Dict, Any
import glob

# Configuration
ENABLE_HF_UPLOAD = os.environ.get("ENABLE_HF_UPLOAD", "0") == "1"
HF_USERNAME = os.environ.get("HF_USERNAME", "your_username")
OUTPUT_DIR = "output"


def create_bioasq_factoid_dataset(source_dataset_name: str):
    """
    Load local parquet files and create test-only bioasq_factoid dataset.

    Args:
        source_dataset_name: Name of source dataset (e.g., "bioasq_trainv0_n1609")
                           Will look for files matching this pattern in output/
    """
    # Build expected filename pattern
    filename_pattern = f"{HF_USERNAME}_{source_dataset_name}_*.parquet"
    pattern_path = os.path.join(OUTPUT_DIR, filename_pattern)

    print(f"Looking for parquet files matching: {pattern_path}")

    # Find matching files
    parquet_files = glob.glob(pattern_path)

    if not parquet_files:
        print(f"ERROR: No parquet files found matching pattern: {filename_pattern}")
        print(f"Please run process_bioasq_golden_answers.py first to generate the files.")
        return None

    print(f"Found {len(parquet_files)} parquet file(s):")
    for f in parquet_files:
        print(f"  - {f}")

    # Load all parquet files and combine
    all_samples = []

    for parquet_file in parquet_files:
        print(f"\nLoading {parquet_file}...")
        df = pd.read_parquet(parquet_file)
        print(f"  Loaded {len(df)} samples")
        print(f"  Columns: {list(df.columns)}")

        # Convert to list of dictionaries
        samples = df.to_dict('records')
        all_samples.extend(samples)

    print(f"\nTotal combined samples: {len(all_samples)}")

    if not all_samples:
        print("ERROR: No samples found in the parquet files!")
        return None

    # Create new dataset with all samples in test set
    print("Creating test-only dataset...")
    test_dataset = Dataset.from_list(all_samples)

    # Create DatasetDict with only test split
    new_dataset = DatasetDict({'test': test_dataset})

    print(f"\nDataset created:")
    print(f"  test: {len(test_dataset)} samples")
    print(f"  Columns: {list(test_dataset.column_names)}")

    # Save locally
    output_name = "bioasq_factoid"
    dataset_name_with_prefix = f"{HF_USERNAME}/{output_name}"
    output_file = os.path.join(OUTPUT_DIR, f"{dataset_name_with_prefix.replace('/', '_')}_test.parquet")

    os.makedirs(OUTPUT_DIR, exist_ok=True)
    test_dataset.to_parquet(output_file)
    print(f"\n✅ Saved test dataset to: {output_file}")

    # Optionally push to HuggingFace Hub
    if ENABLE_HF_UPLOAD:
        print(f"\nPushing dataset to HuggingFace Hub as: {dataset_name_with_prefix}")
        try:
            new_dataset.push_to_hub(dataset_name_with_prefix, token=os.getenv("HF_TOKEN"))
            print(f"✅ Uploaded to: https://huggingface.co/datasets/{dataset_name_with_prefix}")
        except Exception as e:
            print(f"ERROR: Failed to push to HuggingFace: {e}")
            return None
    else:
        print("\nℹ️  To upload to HuggingFace, set ENABLE_HF_UPLOAD=1:")
        print("   export ENABLE_HF_UPLOAD=1")
        print("   export HF_USERNAME=your_username")
        print("   export HF_TOKEN=your_token")

    # Print summary
    print("\n" + "="*60)
    print("SUMMARY")
    print("="*60)
    print(f"Source files: {len(parquet_files)} parquet file(s) from {source_dataset_name}")
    print(f"Output: {output_file}")
    print(f"Total samples in test set: {len(all_samples)}")
    if ENABLE_HF_UPLOAD:
        print(f"HuggingFace: {dataset_name_with_prefix}")

    return new_dataset


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Create test-only BioASQ factoid dataset from local parquet files"
    )
    parser.add_argument(
        "--source_dataset",
        type=str,
        default="bioasq_trainv0_n1609",
        help="Name of source dataset (default: bioasq_trainv0_n1609)"
    )
    args = parser.parse_args()

    create_bioasq_factoid_dataset(args.source_dataset)
