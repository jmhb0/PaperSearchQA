# RAG Baseline for PaperSearchQA

Retrieval-Augmented Generation (RAG) baseline implementation using vLLM for fast batch inference.

## Overview

This RAG implementation:
- Retrieves relevant PubMed abstracts for each question
- Generates answers using retrieved context
- Supports multiple inference methods (direct, CoT, RAG)
- Includes comprehensive caching and evaluation

## Files

- **`batch_rag.py`** - Batch RAG implementation with vLLM
- **`run_inference.py`** - Main evaluation script (direct, CoT, RAG methods)
- **`infer.py`** - SearchR1 inference utilities (single-instance)
- **`infer_searchr1.py`** - BatchSearchR1 for multi-turn search-augmented inference
- **`per_category_results.py`** - Results analysis and category breakdown
- **`qa_em.py`** - Exact match evaluation module

## Architecture

### BatchRAG (`batch_rag.py`)

Core RAG implementation with three main steps:

1. **Batch Retrieval**: Retrieve top-k documents for all questions
   - Calls retrieval server API (localhost:8001)
   - Supports BM25 and dense retrieval (E5)
   - Formats retrieved documents as context

2. **Prompt Construction**: Create RAG prompts with context
   - Applies model-specific chat templates
   - Includes clear answer format instructions

3. **Batch Generation**: Generate answers using vLLM
   - Fast batch inference
   - Configurable sampling parameters
   - Answer extraction from tags

### InferenceEngine (`run_inference.py`)

Main evaluation script supporting:

- **Direct Inference**: Answer without retrieval or reasoning
- **Chain-of-Thought (CoT)**: Step-by-step reasoning without retrieval
- **RAG**: Retrieval + answer generation

Features:
- LMDB caching for all methods
- Exact match evaluation
- Optional LLM judge evaluation
- Batch processing for efficiency

## Prerequisites

### 1. Retrieval Server

Start a retrieval server on localhost:8001:

```bash
# From search-r1/ directory
bash retrieval_launch.sh  # or retrieval_launch_bm25.sh
```

The server must support the `/retrieve` endpoint with format:
```json
{
  "queries": ["question 1?", "question 2?"],
  "topk": 3,
  "return_scores": true
}
```

### 2. Corpus

The retrieval server needs access to the PubMed corpus:

```bash
# Create corpus from allMeSH data (from data_generation/)
python data_generation/core_pipeline/make_pubmed_corpus.py
```

This creates `pubmed.jsonl` with format:
```json
{"id": 0, "contents": "TITLE: ... ABSTRACT: ..."}
```

### 3. Dependencies

```bash
pip install vllm transformers torch datasets pandas numpy tqdm requests lmdb filelock

# For evaluation metrics (verl package)
# Install from search-r1/ directory
```

## Usage

### Basic RAG Evaluation

```bash
python run_inference.py \
    --method rag \
    --model_id Qwen/Qwen2.5-7B-Instruct \
    --dataset_id PaperSearchQA/PaperSearchQA \
    --rag_top_k 3 \
    --retriever_type bm25 \
    --corpus_filename pubmed.jsonl
```

### Direct Inference (No Retrieval)

```bash
python run_inference.py \
    --method direct \
    --model_id Qwen/Qwen2.5-7B-Instruct \
    --dataset_id PaperSearchQA/PaperSearchQA \
    --first_n 100
```

### Chain-of-Thought

```bash
python run_inference.py \
    --method cot \
    --model_id Qwen/Qwen2.5-7B-Instruct \
    --dataset_id PaperSearchQA/PaperSearchQA
```

### With LLM Judge

```bash
python run_inference.py \
    --method rag \
    --model_id Qwen/Qwen2.5-7B-Instruct \
    --dataset_id PaperSearchQA/PaperSearchQA \
    --run_judge
```

## Configuration

### Command Line Arguments

**Dataset Options:**
- `--dataset_id`: HuggingFace dataset ID (default: `jmhb/PaperSearchRL_v4_gv2_n3000_test500`)
- `--first_n`: Number of examples to process (default: 10000)

**Model Options:**
- `--model_id`: Model ID or path (default: `Qwen/Qwen2.5-3B-Instruct`)
- `--max_tokens`: Maximum tokens to generate (default: 1024)
- `--temperature`: Sampling temperature (default: 0.7)
- `--vllm_gpu_memory_utilization`: GPU memory for vLLM (default: 0.9)
- `--vllm_max_model_len`: Max model length (default: 4096)

**RAG-Specific Options:**
- `--rag_top_k`: Top-k documents to retrieve (default: 3)
- `--retriever_type`: Retriever type - `bm25` or `e5` (default: `bm25`)
- `--corpus_filename`: Corpus filename (default: `pubmed.jsonl`)

**Performance Options:**
- `--batch_size`: Inference batch size (default: 1)
- `--no_cache`: Disable caching
- `--overwrite_cache`: Overwrite existing cache

**Evaluation Options:**
- `--run_judge`: Run LLM judge evaluation (requires OpenRouter API key)
- `--output_path`: Custom output path (auto-generated by default)

## Output

Results are saved to `results/eval/run_inference/{dataset_name}/{method}_{model_name}.csv`

### CSV Columns:
- `question`: Original question
- `answer` or `golden_answers`: Ground truth answer(s)
- `model_output`: Full model response
- `prediction`: Extracted answer
- `em_score`: Exact match score (0 or 1)
- `judge_score`: LLM judge score (if --run_judge)
- `judge_explanation`: Judge reasoning
- `judge_judgment`: correct/incorrect/error

### Summary Statistics:
Saved alongside CSV as `.txt` file with:
- Average exact match score
- Average LLM judge score (if applicable)
- Number of examples evaluated

## Answer Format

All methods use `<answer></answer>` tags for consistency:

```
Based on the context, the answer is:

<answer>dystrophin</answer>
```

The `_extract_prediction()` function:
1. Looks for `<answer>...</answer>` tags
2. Falls back to "Answer:" patterns
3. Falls back to first meaningful line

## Caching

Responses are cached in `cache/infer_cache.lmdb` to:
- Avoid redundant API calls
- Speed up repeated evaluations
- Allow resuming interrupted runs

Cache keys include:
- Method, model, and inference parameters
- Input question (for RAG, direct, CoT)

## Exact Match Evaluation

Uses the same methodology as the PaperSearchQA paper:

```python
from verl.utils.reward_score.qa_em import compute_score_em

# Handles multiple acceptable answers
golden_answers = {"target": ["dystrophin", "DMD protein"]}
prediction = "<answer>dystrophin</answer>"
score = compute_score_em(prediction, golden_answers)
```

Score is 1.0 if prediction matches any golden answer, 0.0 otherwise.

## LLM Judge Evaluation

Optional evaluation using GPT-4o as judge:

- Compares prediction to ground truth
- Provides detailed explanation
- Returns correctness score and judgment
- Useful for cases where exact match is too strict

## Performance Tips

1. **First Run**: Cache is empty, so inference will be slow
2. **Subsequent Runs**: Cache hits make it much faster
3. **Memory**: Reduce `--vllm_gpu_memory_utilization` if OOM
4. **Batch Size**: Limited to 1 due to vLLM implementation
5. **Retrieval**: BM25 is faster, E5 is more accurate

## Example Results

```
===========================================
PaperSearchRL Evaluation Summary
===========================================
Method: rag
Model: Qwen/Qwen2.5-7B-Instruct
Dataset: PaperSearchQA/PaperSearchQA
Retriever Type: bm25
Number of examples: 5000

Performance Metrics:
--------------------
Average Exact Match Score: 0.423
===========================================
```

## Dependencies

### Core Dependencies
- `vllm`: Fast LLM inference
- `transformers`: Tokenizers and model loading
- `torch`: PyTorch backend
- `datasets`: HuggingFace datasets
- `pandas`, `numpy`: Data processing

### Retrieval
- `requests`: API calls to retrieval server

### Caching and Concurrency
- `lmdb`: LMDB cache
- `filelock`: File-based locking
- `asyncio`: Async LLM calls for judging

### Evaluation
- `qa_em`: Exact match scoring module (local implementation)

## Troubleshooting

### "Cannot connect to retrieval server"
- Verify server is running: `curl http://localhost:8001/health`
- Check server logs
- Ensure corpus is loaded

### Out of Memory
- Reduce `--vllm_gpu_memory_utilization`
- Use smaller model (3B instead of 7B)
- Process fewer examples with `--first_n`

### Cache Issues
- Use `--overwrite_cache` to force refresh
- Use `--no_cache` to disable caching
- Delete `cache/infer_cache.lmdb` to clear cache

### Import Errors
- Install verl package from search-r1/ directory
- Ensure all dependencies are installed
- Check Python path includes necessary modules

## Notes

- This RAG implementation is adapted from the PaperSearchRL evaluation pipeline
- Answer extraction uses the same format as PaperQA for consistency
- The retrieval server is shared with Search-R1 training infrastructure
